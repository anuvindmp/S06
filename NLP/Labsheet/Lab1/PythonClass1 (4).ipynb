{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eku5QRy49kMM",
        "outputId": "d9ee3d9c-2606-4f39-e385-9009eac3f9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9lGVauacOac",
        "outputId": "463ad4bd-de00-477c-8ccc-c7e324f79b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-DT-ZKQ_j0R"
      },
      "source": [
        "Punkt Sentence Tokenizer. This tokenizer divides a text into a list of\n",
        "sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95InaCU89Ecd"
      },
      "source": [
        "Line Tokenization\n",
        "In the below example we divide a given text into different lines by using the function sent_tokenize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA44RkJiAX1X"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"Mohandas Karamchand Gandhi (ISO: Mōhanadāsa Karamacaṁda Gāṁdhī;[c] 2 October 1869 – 30 January 1948) was an Indian lawyer, anti-colonial nationalist, and political ethicist who employed nonviolent resistance to lead the successful campaign for India's independence from British rule. He inspired movements for civil rights and freedom across the world. The honorific Mahātmā (from Sanskrit 'great-souled, venerable'), first applied to him in South Africa in 1914, is now used throughout the world.\"\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)\n",
        "print(\"length of sentences\",len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_7eMpjxEF3-",
        "outputId": "171625f2-0a0d-4d1f-a574-25289b238ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Mohandas Karamchand Gandhi (ISO: Mōhanadāsa Karamacaṁda Gāṁdhī;[c] 2 October 1869 – 30 January 1948) was an Indian lawyer, anti-colonial nationalist, and political ethicist who employed nonviolent resistance to lead the successful campaign for India's independence from British rule.\", 'He inspired movements for civil rights and freedom across the world.', \"The honorific Mahātmā (from Sanskrit 'great-souled, venerable'), first applied to him in South Africa in 1914, is now used throughout the world.\"]\n",
            "length of sentences 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nITJmKdT83uW",
        "outputId": "d6307a25-d285-464a-bfd8-a12051a044fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of sentences 3\n",
            "Mohandas Karamchand Gandhi (ISO: Mōhanadāsa Karamacaṁda Gāṁdhī;[c] 2 October 1869 – 30 January 1948) was an Indian lawyer, anti-colonial nationalist, and political ethicist who employed nonviolent resistance to lead the successful campaign for India's independence from British rule.\n",
            "['Mohandas', 'Karamchand', 'Gandhi', '(', 'ISO', ':', 'Mōhanadāsa', 'Karamacaṁda', 'Gāṁdhī', ';', '[', 'c', ']', '2', 'October', '1869', '–', '30', 'January', '1948', ')', 'was', 'an', 'Indian', 'lawyer', ',', 'anti-colonial', 'nationalist', ',', 'and', 'political', 'ethicist', 'who', 'employed', 'nonviolent', 'resistance', 'to', 'lead', 'the', 'successful', 'campaign', 'for', 'India', \"'s\", 'independence', 'from', 'British', 'rule', '.']\n",
            "He inspired movements for civil rights and freedom across the world.\n",
            "['He', 'inspired', 'movements', 'for', 'civil', 'rights', 'and', 'freedom', 'across', 'the', 'world', '.']\n",
            "The honorific Mahātmā (from Sanskrit 'great-souled, venerable'), first applied to him in South Africa in 1914, is now used throughout the world.\n",
            "['The', 'honorific', 'Mahātmā', '(', 'from', 'Sanskrit', \"'great-souled\", ',', 'venerable', \"'\", ')', ',', 'first', 'applied', 'to', 'him', 'in', 'South', 'Africa', 'in', '1914', ',', 'is', 'now', 'used', 'throughout', 'the', 'world', '.']\n",
            "90\n"
          ]
        }
      ],
      "source": [
        "totalwords=[]\n",
        "paragraph=\"Mohandas Karamchand Gandhi (ISO: Mōhanadāsa Karamacaṁda Gāṁdhī;[c] 2 October 1869 – 30 January 1948) was an Indian lawyer, anti-colonial nationalist, and political ethicist who employed nonviolent resistance to lead the successful campaign for India's independence from British rule. He inspired movements for civil rights and freedom across the world. The honorific Mahātmā (from Sanskrit 'great-souled, venerable'), first applied to him in South Africa in 1914, is now used throughout the world.\"\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(\"length of sentences\",len(sentences))\n",
        "#print(sentences)\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  words=nltk.word_tokenize(sentence)\n",
        "  print(words)\n",
        "  totalwords=totalwords+words\n",
        "  #print(\"Number of words\",len(words))\n",
        "print(len(totalwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY3KttQCOhK8",
        "outputId": "8449c390-a954-4f6a-a859-ab4c020de8b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mohandas',\n",
              " 'Karamchand',\n",
              " 'Gandhi',\n",
              " '(',\n",
              " 'ISO',\n",
              " ':',\n",
              " 'Mōhanadāsa',\n",
              " 'Karamacaṁda',\n",
              " 'Gāṁdhī',\n",
              " ';',\n",
              " '[',\n",
              " 'c',\n",
              " ']',\n",
              " '2',\n",
              " 'October',\n",
              " '1869',\n",
              " '–',\n",
              " '30',\n",
              " 'January',\n",
              " '1948',\n",
              " ')',\n",
              " 'was',\n",
              " 'an',\n",
              " 'Indian',\n",
              " 'lawyer',\n",
              " ',',\n",
              " 'anti-colonial',\n",
              " 'nationalist',\n",
              " ',',\n",
              " 'and',\n",
              " 'political',\n",
              " 'ethicist',\n",
              " 'who',\n",
              " 'employed',\n",
              " 'nonviolent',\n",
              " 'resistance',\n",
              " 'to',\n",
              " 'lead',\n",
              " 'the',\n",
              " 'successful',\n",
              " 'campaign',\n",
              " 'for',\n",
              " 'India',\n",
              " \"'s\",\n",
              " 'independence',\n",
              " 'from',\n",
              " 'British',\n",
              " 'rule',\n",
              " '.',\n",
              " 'He',\n",
              " 'inspired',\n",
              " 'movements',\n",
              " 'for',\n",
              " 'civil',\n",
              " 'rights',\n",
              " 'and',\n",
              " 'freedom',\n",
              " 'across',\n",
              " 'the',\n",
              " 'world',\n",
              " '.',\n",
              " 'The',\n",
              " 'honorific',\n",
              " 'Mahātmā',\n",
              " '(',\n",
              " 'from',\n",
              " 'Sanskrit',\n",
              " \"'great-souled\",\n",
              " ',',\n",
              " 'venerable',\n",
              " \"'\",\n",
              " ')',\n",
              " ',',\n",
              " 'first',\n",
              " 'applied',\n",
              " 'to',\n",
              " 'him',\n",
              " 'in',\n",
              " 'South',\n",
              " 'Africa',\n",
              " 'in',\n",
              " '1914',\n",
              " ',',\n",
              " 'is',\n",
              " 'now',\n",
              " 'used',\n",
              " 'throughout',\n",
              " 'the',\n",
              " 'world',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "totalwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c52aSzqpk-v5",
        "outputId": "b4b22868-97c0-4b40-a57b-c1c9217ee29b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mohandas', 'Karamchand', 'Gandhi', '(', 'ISO', ':', 'Mōhanadāsa', 'Karamacaṁda', 'Gāṁdhī', ';', '[', 'c', ']', '2', 'October', '1869', '–', '30', 'January', '1948', ')', 'was', 'an', 'Indian', 'lawyer', ',', 'anti-colonial', 'nationalist', ',', 'and', 'political', 'ethicist', 'who', 'employed', 'nonviolent', 'resistance', 'to', 'lead', 'the', 'successful', 'campaign', 'for', 'India', \"'s\", 'independence', 'from', 'British', 'rule', '.']\n",
            "Mohandas\n",
            "Karamchand\n",
            "Gandhi\n",
            "(\n",
            "ISO\n",
            ":\n",
            "Mōhanadāsa\n",
            "Karamacaṁda\n",
            "Gāṁdhī\n",
            ";\n",
            "[\n",
            "c\n",
            "]\n",
            "2\n",
            "October\n",
            "1869\n",
            "–\n",
            "30\n",
            "January\n",
            "1948\n",
            ")\n",
            "was\n",
            "an\n",
            "Indian\n",
            "lawyer\n",
            ",\n",
            "anti-colonial\n",
            "nationalist\n",
            ",\n",
            "and\n",
            "political\n",
            "ethicist\n",
            "who\n",
            "employed\n",
            "nonviolent\n",
            "resistance\n",
            "to\n",
            "lead\n",
            "the\n",
            "successful\n",
            "campaign\n",
            "for\n",
            "India\n",
            "'s\n",
            "independence\n",
            "from\n",
            "British\n",
            "rule\n",
            ".\n",
            "['He', 'inspired', 'movements', 'for', 'civil', 'rights', 'and', 'freedom', 'across', 'the', 'world', '.']\n",
            "He\n",
            "inspired\n",
            "movements\n",
            "for\n",
            "civil\n",
            "rights\n",
            "and\n",
            "freedom\n",
            "across\n",
            "the\n",
            "world\n",
            ".\n",
            "['The', 'honorific', 'Mahātmā', '(', 'from', 'Sanskrit', \"'great-souled\", ',', 'venerable', \"'\", ')', ',', 'first', 'applied', 'to', 'him', 'in', 'South', 'Africa', 'in', '1914', ',', 'is', 'now', 'used', 'throughout', 'the', 'world', '.']\n",
            "The\n",
            "honorific\n",
            "Mahātmā\n",
            "(\n",
            "from\n",
            "Sanskrit\n",
            "'great-souled\n",
            ",\n",
            "venerable\n",
            "'\n",
            ")\n",
            ",\n",
            "first\n",
            "applied\n",
            "to\n",
            "him\n",
            "in\n",
            "South\n",
            "Africa\n",
            "in\n",
            "1914\n",
            ",\n",
            "is\n",
            "now\n",
            "used\n",
            "throughout\n",
            "the\n",
            "world\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "for sent in sentences:\n",
        "  words=nltk.word_tokenize(sent)\n",
        "  print(words)\n",
        "  for wd in words:\n",
        "    print(wd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n10cs1tDAaaR"
      },
      "source": [
        "**Word Tokenzitaion**\n",
        "We tokenize the words using word_tokenize function available as part of nltk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwP-5xGe9Dk8",
        "outputId": "30085635-177d-445d-8da8-0a204efc1aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "It\n",
            "originated\n",
            "from\n",
            "the\n",
            "idea\n",
            "that\n",
            "there\n",
            "are\n",
            "readers\n",
            "who\n",
            "prefer\n",
            "learning\n",
            "new\n",
            "skills\n",
            "from\n",
            "the\n",
            "comforts\n",
            "of\n",
            "their\n",
            "drawing\n",
            "rooms\n"
          ]
        }
      ],
      "source": [
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "print(len(nltk_tokens))\n",
        "for i in nltk_tokens:\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi-R4hu2DiLc",
        "outputId": "fe57fdd5-6593-4c35-bd97-5d1b446485f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_stopwords=stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r70JxtMtj4PD",
        "outputId": "10fb212c-916f-4626-9eb8-19f73160ff4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ],
      "source": [
        "print(len(stop_stopwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6vBAkFfiPJE",
        "outputId": "0d46f3bd-1ba6-414e-d1f1-1822f25fb8ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "stop_stopwords[10:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbJLMSaQQL0a",
        "outputId": "0d931c1c-f572-47fb-96f7-f103776b4719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Loamy', 'soil', 'is', 'a', 'mixture', 'of', 'sand', ',', 'silt', 'and', 'clay', 'which', 'provides', 'ideal', 'nutrition', 'for', 'wheat', '.', 'With', 'high', 'draining', 'capacity', ',', 'Loamy', 'soil', 'can', 'hold', 'plenty', 'of', 'water', 'suitable', 'for', 'growing', 'lentils', '(', 'masoor', ')', ',', 'cotton', ',', 'and', 'other', 'pulses', '.', 'Alluvial', 'sandy', 'loam', ',', 'clay', 'loamy', 'soils', 'are', 'best', 'suited', 'for', 'jute', 'production', '.', 'Therefore', ',', 'loamy', 'soil', 'is', 'preferred', 'for', 'growing', 'pulses', '.', 'Loamy', 'soil', 'is', 'ideal', 'for', 'growing', 'several', 'crops', 'that', 'are', 'wheat', ',', 'sugarcane', ',', 'cotton', ',', 'pulses', ',', 'and', 'oilseeds', '.', 'Because', 'of', 'their', 'clayey', 'nature', ',', 'black', 'soil', 'is', 'much', 'required', 'for', 'growing', 'cotton', '.', 'Alluvial', 'soils', 'are', 'ideal', 'for', 'the', 'growth', 'of', 'sugarcane', ',', 'paddy', ',', 'wheat', 'and', 'other', 'cereal', 'and', 'pulse', 'crops', '.', 'Super', 'Sandy', 'Soil', ':', 'If', 'your', 'soil', 'is', 'very', 'sandy', ',', 'you', 'will', 'probably', 'have', 'problems', 'growing', 'hibiscus', 'in', 'it', '.', 'Carrots', 'are', 'grown', 'in', 'sandy', 'soil', 'because', 'they', 'need', 'a', 'loose', 'medium', '(', 'soil', ')', 'free', 'from', 'stones', 'and', 'clumps', 'to', 'form', 'uniformly', 'shaped', 'carrots', '.', 'Sweet', 'corn', 'crops', 'grown', 'on', 'sandy', 'soils', 'depend', 'on', 'proper', 'nitrogen', '(', 'N', ')', 'fertilizer', 'inputs', 'and', 'management', '.', 'Lettuce', 'is', 'a', 'leafy', 'vegetable', 'that', 'can', 'thrive', 'in', 'sandy', 'soil', '.', 'Potatoes', 'grow', 'best', 'in', 'well-drained', ',', 'sandy', 'soil', '.', 'Lettuce', ',', 'strawberries', ',', 'peppers', ',', 'corn', ',', 'squash', ',', 'zucchini', ',', 'collard', 'greens', 'and', 'tomatoes', 'are', 'grown', 'commercially', 'in', 'sandy', 'soils', '.', 'Wheat', 'is', 'grown', 'in', 'the', 'clayey', 'soil', 'because', '.', 'Answer', ':', 'Solution', '5', ':', 'Clayey', 'soil', 'is', 'rich', 'in', 'humus', 'and', 'very', 'fertile', ',', 'so', 'it', 'is', 'suitable', 'for', 'growing', 'cereals', 'like', 'wheat', 'and', 'gram', '.', 'The', 'best', 'soil', 'for', 'growing', 'paddy', 'is', 'the', 'Clayey', 'soil', 'as', 'they', 'can', 'hold', 'the', 'water', 'due', 'to', 'their', 'very', 'fine', 'particle', 'size', '.', 'Because', 'of', 'their', 'clayey', 'nature', ',', 'black', 'soil', 'is', 'much', 'required', 'for', 'growing', 'cotton', '.', 'Alluvial', 'soils', 'are', 'ideal', 'for', 'the', 'growth', 'of', 'sugarcane', ',', 'paddy', ',', 'wheat', 'and', 'other', 'cereal', 'and', 'pulse', 'crops', '.', 'Super', 'Sandy', 'Soil', ':', 'If', 'your', 'soil', 'is', 'very', 'sandy', ',', 'you', 'will', 'probably', 'have', 'problems', 'growing', 'hibiscus', 'in', 'it', '.', 'Carrots', 'are', 'grown', 'in', 'sandy', 'soil', 'because', 'they', 'need', 'a', 'loose', 'medium', '(', 'soil', ')', 'free', 'from', 'stones', 'and', 'clumps', 'to', 'form', 'uniformly', 'shaped', 'carrots', '.', 'Corn', 'planted', 'in', 'sandy', 'soils', 'may', 'be', 'planted', 'as', 'deep', 'as', '3', 'inches', '.', 'Lettuce', 'is', 'a', 'leafy', 'vegetable', 'that', 'can', 'thrive', 'in', 'sandy', 'soil', '.', 'Potatoes', 'grow', 'best', 'in', 'well-drained', ',', 'sandy', 'soil', '.', 'Sandy', 'soil', 'in', 'and', 'of', 'itself', 'is', 'not', 'a', 'bad', 'thing', ',', 'but', 'peppers', 'do', 'appreciate', 'an', 'evenly', 'moist', 'and', 'yet', 'well', 'drained', 'soil', '.', 'Wheat', 'is', 'grown', 'in', 'the', 'clayey', 'soil', 'because', '.', 'D.', 'Loamy', 'and', 'clayey', 'soil', '.', 'Clayey', 'soil', 'is', 'rich', 'in', 'humus', 'and', 'very', 'fertile', ',', 'so', 'it', 'is', 'suitable', 'for', 'growing', 'cereals', 'like', 'wheat', 'and', 'gram', '.', 'The', 'best', 'soil', 'for', 'growing', 'paddy', 'is', 'the', 'Clayey', 'soil', 'as', 'they', 'can', 'hold', 'the', 'water', 'due', 'to', 'their', 'very', 'fine', 'particle', 'size', '.', 'Clayey', 'and', 'loamy', 'soils', 'are', 'both', 'suitable', 'for', 'growing', 'cereals', 'like', 'wheat', ',', 'and', 'gram', '.', 'by', 'S', 'Karimi', '·', '2022', '·', 'Cited', 'by', '1', '—', 'Based', 'on', 'the', 'results', ',', 'all', 'loamy', 'soil', 'samples', 'mixed', 'with', 'sugarcane', 'molasses', 'showed', 'a', 'significant', 'increase', 'in', 'shear', 'strength', '.', 'Loamy', 'soil', 'is', 'ideal', 'for', 'growing', 'several', 'crops', 'that', 'are', 'wheat', ',', 'sugarcane', ',', 'cotton', ',', 'pulses', ',', 'and', 'oilseeds', '.', 'Alluvial', 'sandy', 'loam', ',', 'clay', 'loamy', 'soils', 'are', 'best', 'suited', 'for', 'jute', 'production', '.', 'Therefore', ',', 'loamy', 'soil', 'is', 'preferred', 'for', 'growing', 'pulses', '.', 'Loamy', 'soil', 'is', 'ideal', 'for', 'growing', 'several', 'crops', 'that', 'are', 'wheat', ',', 'sugarcane', ',', 'cotton', ',', 'pulses', ',', 'and', 'oilseeds', 'Deep', ',', 'rich', 'loamy', 'soil', 'with', 'pH', 'between', '6.5', '–', '7.5', 'is', 'most', 'preferred', 'for', 'banana', 'cultivation', '.']\n"
          ]
        }
      ],
      "source": [
        "print(totalwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpoSbOgREYmQ",
        "outputId": "508d5db7-10a4-4a3f-8601-c7f2a4bf9569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of non stopwords 441\n",
            "Number of stopwords 186\n",
            "['is', 'a', 'of', 'and', 'which', 'for', 'can', 'of', 'for', 'and', 'other', 'are', 'for', 'is', 'for', 'is', 'for', 'that', 'are', 'and', 'of', 'their', 'is', 'for', 'are', 'for', 'the', 'of', 'and', 'other', 'and', 'your', 'is', 'very', 'you', 'will', 'have', 'in', 'it', 'are', 'in', 'because', 'they', 'a', 'from', 'and', 'to', 'on', 'on', 'and', 'is', 'a', 'that', 'can', 'in', 'in', 'and', 'are', 'in', 'is', 'in', 'the', 'because', 'is', 'in', 'and', 'very', 'so', 'it', 'is', 'for', 'and', 'for', 'is', 'the', 'as', 'they', 'can', 'the', 'to', 'their', 'very', 'of', 'their', 'is', 'for', 'are', 'for', 'the', 'of', 'and', 'other', 'and', 'your', 'is', 'very', 'you', 'will', 'have', 'in', 'it', 'are', 'in', 'because', 'they', 'a', 'from', 'and', 'to', 'in', 'be', 'as', 'as', 'is', 'a', 'that', 'can', 'in', 'in', 'in', 'and', 'of', 'itself', 'is', 'not', 'a', 'but', 'do', 'an', 'and', 'is', 'in', 'the', 'because', 'and', 'is', 'in', 'and', 'very', 'so', 'it', 'is', 'for', 'and', 'for', 'is', 'the', 'as', 'they', 'can', 'the', 'to', 'their', 'very', 'and', 'are', 'both', 'for', 'and', 'by', 'by', 'on', 'the', 'all', 'with', 'a', 'in', 'is', 'for', 'that', 'are', 'and', 'are', 'for', 'is', 'for', 'is', 'for', 'that', 'are', 'and', 'with', 'between', 'is', 'most', 'for']\n"
          ]
        }
      ],
      "source": [
        "stopwords=[]\n",
        "nonstopwords=[]\n",
        "for word in totalwords:\n",
        "    if word not in stop_stopwords:\n",
        "        nonstopwords.append(word)\n",
        "    else:\n",
        "         stopwords.append(word)\n",
        "print(\"Number of non stopwords\", len(nonstopwords))\n",
        "print(\"Number of stopwords\", len(stopwords))\n",
        "print(stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa-mwsztOXVl",
        "outputId": "148dc8ba-fd86-44a7-aefa-33ad7b79e871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQmGphLAWYE5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "k0TKnjxHSNPV",
        "outputId": "37296b6b-511a-4df0-8d5f-832a30a8efba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e156bb4e-481c-4689-a2f6-9ac49b560e76\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e156bb4e-481c-4689-a2f6-9ac49b560e76\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving soil_crop_sentences.txt to soil_crop_sentences.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KSDqRobS4nT",
        "outputId": "d8eba0b7-0662-406a-c170-ba50f8abd615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OufLboV1VpPf",
        "outputId": "ebc89d7a-5295-4684-88a2-02caab314978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "627\n",
            "189\n"
          ]
        }
      ],
      "source": [
        "totalwords=[]\n",
        "f = open('soil_crop_sentences.txt', 'r', encoding='utf8', errors='ignore')\n",
        "lines = f.read()\n",
        "sentences=nltk.sent_tokenize(lines)\n",
        "for sent in sentences:\n",
        "    totalwords=totalwords+nltk.word_tokenize(sent)\n",
        "print(len(totalwords))\n",
        "print(len(set(totalwords)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTq0XpFPWA90",
        "outputId": "3a42856d-b6a0-4048-db78-d4380b029c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n"
          ]
        }
      ],
      "source": [
        "nltk_tokens = nltk.sent_tokenize(lines)\n",
        "print(len(nltk_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFZo0Nr6YYrD",
        "outputId": "252d32cb-84de-42b3-c629-836188419e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "627\n"
          ]
        }
      ],
      "source": [
        "nltk_tokens = nltk.word_tokenize(lines)\n",
        "print(len(nltk_tokens))\n",
        "tokens=nltk.word_tokenize(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA3oaUyM59Wk",
        "outputId": "f357240e-af44-4c20-a796-1bdfd73796f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "339\n"
          ]
        }
      ],
      "source": [
        "print(len(set(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt3NxCPJCfFr",
        "outputId": "a2cc1296-50cb-40ba-eea3-7139dfc75dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loamy\n",
            "soil\n",
            "mixture\n",
            "sand\n",
            "silt\n",
            "clay\n",
            "provides\n",
            "ideal\n",
            "nutrition\n",
            "wheat\n",
            "With\n",
            "high\n",
            "draining\n",
            "capacity\n",
            "Loamy\n",
            "soil\n",
            "hold\n",
            "plenty\n",
            "water\n",
            "suitable\n",
            "growing\n",
            "lentils\n",
            "(\n",
            "masoor\n",
            ")\n",
            "cotton\n",
            "pulses\n",
            "Alluvial\n",
            "sandy\n",
            "loam\n",
            "clay\n",
            "loamy\n",
            "soils\n",
            "best\n",
            "suited\n",
            "jute\n",
            "production\n",
            "Therefore\n",
            "loamy\n",
            "soil\n",
            "preferred\n",
            "growing\n",
            "pulses\n",
            "Loamy\n",
            "soil\n",
            "ideal\n",
            "growing\n",
            "several\n",
            "crops\n",
            "wheat\n",
            "sugarcane\n",
            "cotton\n",
            "pulses\n",
            "oilseeds\n",
            "Because\n",
            "clayey\n",
            "nature\n",
            "black\n",
            "soil\n",
            "much\n",
            "required\n",
            "growing\n",
            "cotton\n",
            "Alluvial\n",
            "soils\n",
            "ideal\n",
            "growth\n",
            "sugarcane\n",
            "paddy\n",
            "wheat\n",
            "cereal\n",
            "pulse\n",
            "crops\n",
            "Super\n",
            "Sandy\n",
            "Soil\n",
            ":\n",
            "If\n",
            "soil\n",
            "sandy\n",
            "probably\n",
            "problems\n",
            "growing\n",
            "hibiscus\n",
            "Carrots\n",
            "grown\n",
            "sandy\n",
            "soil\n",
            "need\n",
            "loose\n",
            "medium\n",
            "(\n",
            "soil\n",
            ")\n",
            "free\n",
            "stones\n",
            "clumps\n",
            "form\n",
            "uniformly\n",
            "shaped\n",
            "carrots\n",
            "Sweet\n",
            "corn\n",
            "crops\n",
            "grown\n",
            "sandy\n",
            "soils\n",
            "depend\n",
            "proper\n",
            "nitrogen\n",
            "(\n",
            "N\n",
            ")\n",
            "fertilizer\n",
            "inputs\n",
            "management\n",
            "Lettuce\n",
            "leafy\n",
            "vegetable\n",
            "thrive\n",
            "sandy\n",
            "soil\n",
            "Potatoes\n",
            "grow\n",
            "best\n",
            "well-drained\n",
            "sandy\n",
            "soil\n",
            "Lettuce\n",
            "strawberries\n",
            "peppers\n",
            "corn\n",
            "squash\n",
            "zucchini\n",
            "collard\n",
            "greens\n",
            "tomatoes\n",
            "grown\n",
            "commercially\n",
            "sandy\n",
            "soils\n",
            "Wheat\n",
            "grown\n",
            "clayey\n",
            "soil\n",
            "Answer\n",
            ":\n",
            "Solution\n",
            "5\n",
            ":\n",
            "Clayey\n",
            "soil\n",
            "rich\n",
            "humus\n",
            "fertile\n",
            "suitable\n",
            "growing\n",
            "cereals\n",
            "like\n",
            "wheat\n",
            "gram\n",
            "The\n",
            "best\n",
            "soil\n",
            "growing\n",
            "paddy\n",
            "Clayey\n",
            "soil\n",
            "hold\n",
            "water\n",
            "due\n",
            "fine\n",
            "particle\n",
            "size\n",
            "Because\n",
            "clayey\n",
            "nature\n",
            "black\n",
            "soil\n",
            "much\n",
            "required\n",
            "growing\n",
            "cotton\n",
            "Alluvial\n",
            "soils\n",
            "ideal\n",
            "growth\n",
            "sugarcane\n",
            "paddy\n",
            "wheat\n",
            "cereal\n",
            "pulse\n",
            "crops\n",
            "Super\n",
            "Sandy\n",
            "Soil\n",
            ":\n",
            "If\n",
            "soil\n",
            "sandy\n",
            "probably\n",
            "problems\n",
            "growing\n",
            "hibiscus\n",
            "Carrots\n",
            "grown\n",
            "sandy\n",
            "soil\n",
            "need\n",
            "loose\n",
            "medium\n",
            "(\n",
            "soil\n",
            ")\n",
            "free\n",
            "stones\n",
            "clumps\n",
            "form\n",
            "uniformly\n",
            "shaped\n",
            "carrots\n",
            "Corn\n",
            "planted\n",
            "sandy\n",
            "soils\n",
            "may\n",
            "planted\n",
            "deep\n",
            "3\n",
            "inches\n",
            "Lettuce\n",
            "leafy\n",
            "vegetable\n",
            "thrive\n",
            "sandy\n",
            "soil\n",
            "Potatoes\n",
            "grow\n",
            "best\n",
            "well-drained\n",
            "sandy\n",
            "soil\n",
            "Sandy\n",
            "soil\n",
            "bad\n",
            "thing\n",
            "peppers\n",
            "appreciate\n",
            "evenly\n",
            "moist\n",
            "yet\n",
            "well\n",
            "drained\n",
            "soil\n",
            "Wheat\n",
            "grown\n",
            "clayey\n",
            "soil\n",
            "D.\n",
            "Loamy\n",
            "clayey\n",
            "soil\n",
            "Clayey\n",
            "soil\n",
            "rich\n",
            "humus\n",
            "fertile\n",
            "suitable\n",
            "growing\n",
            "cereals\n",
            "like\n",
            "wheat\n",
            "gram\n",
            "The\n",
            "best\n",
            "soil\n",
            "growing\n",
            "paddy\n",
            "Clayey\n",
            "soil\n",
            "hold\n",
            "water\n",
            "due\n",
            "fine\n",
            "particle\n",
            "size\n",
            "Clayey\n",
            "loamy\n",
            "soils\n",
            "suitable\n",
            "growing\n",
            "cereals\n",
            "like\n",
            "wheat\n",
            "gram\n",
            "S\n",
            "Karimi\n",
            "·\n",
            "2022\n",
            "·\n",
            "Cited\n",
            "1\n",
            "—\n",
            "Based\n",
            "results\n",
            "loamy\n",
            "soil\n",
            "samples\n",
            "mixed\n",
            "sugarcane\n",
            "molasses\n",
            "showed\n",
            "significant\n",
            "increase\n",
            "shear\n",
            "strength\n",
            "Loamy\n",
            "soil\n",
            "ideal\n",
            "growing\n",
            "several\n",
            "crops\n",
            "wheat\n",
            "sugarcane\n",
            "cotton\n",
            "pulses\n",
            "oilseeds\n",
            "Alluvial\n",
            "sandy\n",
            "loam\n",
            "clay\n",
            "loamy\n",
            "soils\n",
            "best\n",
            "suited\n",
            "jute\n",
            "production\n",
            "Therefore\n",
            "loamy\n",
            "soil\n",
            "preferred\n",
            "growing\n",
            "pulses\n",
            "Loamy\n",
            "soil\n",
            "ideal\n",
            "growing\n",
            "several\n",
            "crops\n",
            "wheat\n",
            "sugarcane\n",
            "cotton\n",
            "pulses\n",
            "oilseeds\n",
            "Deep\n",
            "rich\n",
            "loamy\n",
            "soil\n",
            "pH\n",
            "6.5\n",
            "–\n",
            "7.5\n",
            "preferred\n",
            "banana\n",
            "cultivation\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "#all_words = ['There', 'is', 'a', 'tree','near','the','river']\n",
        "for word in tokens:\n",
        "    if word not in stop_stopwords:\n",
        "        print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUcN3RyKCxxc"
      },
      "outputs": [],
      "source": [
        "stop_stopwords.extend([',',';','also','.'])\n",
        "tokens = nltk.tokenize.word_tokenize(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuCD3MBYK6RG",
        "outputId": "02f20e21-6471-4830-c8e7-28458e2c8631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 189 samples and 627 outcomes>\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "fdist=FreqDist(totalwords)\n",
        "print(fdist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnE2kUsNoPQO",
        "outputId": "768e9618-1270-4c42-cbf4-9c4b244fc3e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj2uc3lWMxw2",
        "outputId": "23d9f6ca-6cba-4f78-d01b-e40abc52b8f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'best'), ('best', 'performance'), ('performance', 'can'), ('can', 'bring'), ('bring', 'in'), ('in', 'sky'), ('sky', 'high'), ('high', 'success'), ('success', '.')]\n",
            "[('The', 'best', 'performance'), ('best', 'performance', 'can'), ('performance', 'can', 'bring'), ('can', 'bring', 'in'), ('bring', 'in', 'sky'), ('in', 'sky', 'high'), ('sky', 'high', 'success'), ('high', 'success', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "word_data = \"The best performance can bring in sky high success.\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "print(list(nltk.bigrams(nltk_tokens)))\n",
        "print(list(nltk.trigrams(nltk_tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WFlI_BoaFQa"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "fdist=FreqDist(totalwords)\n",
        "print(fdist)\n",
        "fdist.most_common(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDKNnKuUyuvb"
      },
      "outputs": [],
      "source": [
        "fdist.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CyNlvKF0SAP",
        "outputId": "a0ca10ec-7d02-4e6e-835e-c016638264be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           ,            .         soil           is          and          for           in      growing        sandy          are        wheat          the        soils            a           of        loamy        Loamy        ideal       cotton       pulses         best        crops    sugarcane         very        grown          can         that       clayey       Clayey     suitable            (            )     Alluvial        their        paddy            :           it      because         they           to           as         clay         hold        water        other    preferred      several     oilseeds        Sandy           on      Lettuce         rich      cereals         like         gram         loam       suited         jute   production    Therefore      Because       nature        black         much     required       growth       cereal        pulse        Super         Soil           If         your          you         will     probably         have     problems     hibiscus      Carrots         need        loose       medium         free         from       stones       clumps         form    uniformly       shaped      carrots         corn        leafy    vegetable       thrive     Potatoes         grow well-drained      peppers        Wheat        humus      fertile           so          The          due         fine     particle         size      planted           by            ·         with      mixture         sand         silt        which     provides    nutrition         With         high     draining     capacity       plenty      lentils       masoor        Sweet       depend       proper     nitrogen            N   fertilizer       inputs   management strawberries       squash     zucchini      collard       greens     tomatoes commercially       Answer     Solution            5         Corn          may           be         deep            3       inches       itself          not          bad        thing          but           do   appreciate           an       evenly        moist          yet         well      drained           D.         both            S       Karimi         2022        Cited            1            —        Based      results          all      samples        mixed     molasses       showed  significant     increase        shear     strength         Deep           pH      between          6.5            –          7.5         most       banana  cultivation \n",
            "          42           34           32           22           22           19           16           15           13           11            9            9            8            7            7            7            6            6            6            6            6            6            6            6            6            5            5            5            5            4            4            4            4            4            4            4            4            4            4            4            4            3            3            3            3            3            3            3            3            3            3            3            3            3            3            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            2            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1            1 \n"
          ]
        }
      ],
      "source": [
        "fdist.tabulate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK-PvSLk0ewQ",
        "outputId": "79dd5f03-37dc-4cd0-f399-3202b9f7805b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "fdist.N()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznYG52iCNIy",
        "outputId": "ba4b951d-c67a-4162-aacb-14e6f5f8726e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19753086419753085"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "fdist.freq('soil')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "id": "GK9DVzH6amHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PZgxV4uOaYL",
        "outputId": "6ec05a53-026e-4f8e-aac8-fbbb4e132abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wlak\n",
            "walk\n",
            "{'weak', 'walk', 'flak'}\n",
            "grou\n",
            "grow\n",
            "{'grout', 'gros', 'grot', 'grog', 'grow', 'group'}\n",
            "lette\n",
            "letter\n",
            "{'bette', 'lethe', 'latte', 'leyte', 'letter', 'leste', 'lett'}\n"
          ]
        }
      ],
      "source": [
        "pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "misspelled = spell.unknown(['lette', 'us', 'wlak','on','the','grou'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(word)\n",
        "    print(spell.correction(word))\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "#proide a word to be stemmed\n",
        "print(\"Porter Stemmer\")\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"trouble\"))\n",
        "print(porter.stem(\"troubling\"))\n",
        "print(porter.stem(\"troubled\"))\n",
        "print(\"Lancaster Stemmer\")\n",
        "print(lancaster.stem(\"cats\"))\n",
        "print(lancaster.stem(\"trouble\"))\n",
        "print(lancaster.stem(\"troubling\"))\n",
        "print(lancaster.stem(\"are\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIT_pLhcatVa",
        "outputId": "781917d1-5d28-49e0-b3cc-02b52ff2c8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "Lancaster Stemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "ar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUv53FggavKS",
        "outputId": "423e8c29-01d4-4872-ca50-acd42683962c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      lancaster Stemmer   \n",
            "friend              friend              friend              \n",
            "friendship          friendship          friend              \n",
            "friends             friend              friend              \n",
            "friendships         friendship          friend              \n",
            "stabil              stabil              stabl               \n",
            "destabilize         destabil            dest                \n",
            "misunderstanding    misunderstand       misunderstand       \n",
            "railroad            railroad            railroad            \n",
            "moonlight           moonlight           moonlight           \n",
            "football            footbal             footbal             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Ss1FfUbQFr",
        "outputId": "ff9adcdd-5cbe-4d4b-cf20-78792ff68267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\"))\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\",pos='a'))\n",
        "print(\"are :\", lemmatizer.lemmatize(\"are\",pos='n'))\n",
        "print(\"is :\", lemmatizer.lemmatize(\"is\",pos='v'))\n",
        "print(\"am :\", lemmatizer.lemmatize(\"am\",pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OznybAGBbJuK",
        "outputId": "288bffc6-0bb4-4ffe-84e0-2f4eba961e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : better\n",
            "better : good\n",
            "are : are\n",
            "is : be\n",
            "am : be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3jFBwp4bTkD",
        "outputId": "49461df8-0698-4f60-e1b7-2adbbbc79408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5MBJLFU85o0",
        "outputId": "5e3a788a-5ffe-40e9-e169-2048bab50382"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# sample text\n",
        "sample = gutenberg.raw(\"austen-sense.txt\")\n",
        "\n",
        "tok = sent_tokenize(sample)\n",
        "\n",
        "for x in range(5):\n",
        "    print(tok[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesASUbjbgQE",
        "outputId": "626580a0-dccc-4446-8ef8-235b151eacae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sense and Sensibility by Jane Austen 1811]\n",
            "\n",
            "CHAPTER 1\n",
            "\n",
            "\n",
            "The family of Dashwood had long been settled in Sussex.\n",
            "Their estate was large, and their residence was at Norland Park,\n",
            "in the centre of their property, where, for many generations,\n",
            "they had lived in so respectable a manner as to engage\n",
            "the general good opinion of their surrounding acquaintance.\n",
            "The late owner of this estate was a single man, who lived\n",
            "to a very advanced age, and who for many years of his life,\n",
            "had a constant companion and housekeeper in his sister.\n",
            "But her death, which happened ten years before his own,\n",
            "produced a great alteration in his home; for to supply\n",
            "her loss, he invited and received into his house the family\n",
            "of his nephew Mr. Henry Dashwood, the legal inheritor\n",
            "of the Norland estate, and the person to whom he intended\n",
            "to bequeath it.\n",
            "In the society of his nephew and niece,\n",
            "and their children, the old Gentleman's days were\n",
            "comfortably spent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=gutenberg.words(\"milton-paradise.txt\")\n",
        "len(set(all_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdve7wh2b3QD",
        "outputId": "ae9d36d1-a162-44a8-8d63-365996d20a88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10751"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_words[30:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2yt00FJb4zr",
        "outputId": "8b10a90b-ef1a-449f-e803-d2e2e89dc4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'World', ',', 'and', 'all', 'our', 'woe', ',', 'With', 'loss', 'of', 'Eden', ',', 'till', 'one', 'greater', 'Man', 'Restore', 'us', ',']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fileid in gutenberg.fileids():\n",
        "    num_words = len(gutenberg.words(fileid))\n",
        "    num_sents = len(gutenberg.sents(fileid))\n",
        "    print(\"Data for file:\", fileid)\n",
        "    print(\"Number of words:\", num_words)\n",
        "    print(\"Number of sentences:\", num_sents, end=\"\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T06ZCbTRcGDT",
        "outputId": "ccee2e08-927e-44ce-a878-abfec797175b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for file: austen-emma.txt\n",
            "Number of words: 192427\n",
            "Number of sentences: 7752\n",
            "\n",
            "\n",
            "Data for file: austen-persuasion.txt\n",
            "Number of words: 98171\n",
            "Number of sentences: 3747\n",
            "\n",
            "\n",
            "Data for file: austen-sense.txt\n",
            "Number of words: 141576\n",
            "Number of sentences: 4999\n",
            "\n",
            "\n",
            "Data for file: bible-kjv.txt\n",
            "Number of words: 1010654\n",
            "Number of sentences: 30103\n",
            "\n",
            "\n",
            "Data for file: blake-poems.txt\n",
            "Number of words: 8354\n",
            "Number of sentences: 438\n",
            "\n",
            "\n",
            "Data for file: bryant-stories.txt\n",
            "Number of words: 55563\n",
            "Number of sentences: 2863\n",
            "\n",
            "\n",
            "Data for file: burgess-busterbrown.txt\n",
            "Number of words: 18963\n",
            "Number of sentences: 1054\n",
            "\n",
            "\n",
            "Data for file: carroll-alice.txt\n",
            "Number of words: 34110\n",
            "Number of sentences: 1703\n",
            "\n",
            "\n",
            "Data for file: chesterton-ball.txt\n",
            "Number of words: 96996\n",
            "Number of sentences: 4779\n",
            "\n",
            "\n",
            "Data for file: chesterton-brown.txt\n",
            "Number of words: 86063\n",
            "Number of sentences: 3806\n",
            "\n",
            "\n",
            "Data for file: chesterton-thursday.txt\n",
            "Number of words: 69213\n",
            "Number of sentences: 3742\n",
            "\n",
            "\n",
            "Data for file: edgeworth-parents.txt\n",
            "Number of words: 210663\n",
            "Number of sentences: 10230\n",
            "\n",
            "\n",
            "Data for file: melville-moby_dick.txt\n",
            "Number of words: 260819\n",
            "Number of sentences: 10059\n",
            "\n",
            "\n",
            "Data for file: milton-paradise.txt\n",
            "Number of words: 96825\n",
            "Number of sentences: 1851\n",
            "\n",
            "\n",
            "Data for file: shakespeare-caesar.txt\n",
            "Number of words: 25833\n",
            "Number of sentences: 2163\n",
            "\n",
            "\n",
            "Data for file: shakespeare-hamlet.txt\n",
            "Number of words: 37360\n",
            "Number of sentences: 3106\n",
            "\n",
            "\n",
            "Data for file: shakespeare-macbeth.txt\n",
            "Number of words: 23140\n",
            "Number of sentences: 1907\n",
            "\n",
            "\n",
            "Data for file: whitman-leaves.txt\n",
            "Number of words: 154883\n",
            "Number of sentences: 4250\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')\n",
        "sents=nltk.corpus.brown.tagged_words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF4w2DuMcPAJ",
        "outputId": "44aed45a-f8d2-4a87-ddc0-286f99b92b9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "# Sample sentence\n",
        "text = \"Barack Obama was the 44th President of the United States.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "ner_tree = ne_chunk(pos_tags)\n",
        "print(\"Named Entities:\")\n",
        "print(ner_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faDhN-kxhAEl",
        "outputId": "54fa8eb3-9f4f-4981-97d6-fe27f2a44539"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('44th', 'JJ'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n",
            "Named Entities:\n",
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  44th/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}